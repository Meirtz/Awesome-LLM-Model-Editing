## 1. Title: GenTKG: Generative Forecasting on Temporal Knowledge Graph

- **Authors**: Ruotong Liao, Xu Jia, Yunpu Ma, Volker Tresp
- **Link**: https://arxiv.org/pdf/2310.07793

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 2. Title: Exploring the Relationship between Analogy Identification and Sentence  Structure Encoding in Large Language Models

- **Authors**: Thilini Wijesiriwardene, Ruwan Wickramarachchi, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das
- **Link**: https://arxiv.org/pdf/2310.07818

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 3. Title: Faithfulness Measurable Masked Language Models

- **Authors**: Andreas Madsen, Siva Reddy, Sarath Chandar
- **Link**: https://arxiv.org/pdf/2310.07819

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 4. Title: Does Synthetic Data Make Large Language Models More Efficient?

- **Authors**: Sia Gholami, Marwan Omar
- **Link**: https://arxiv.org/pdf/2310.07830

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 5. Title: Synthetic Data Generation with Large Language Models for Text  Classification: Potential and Limitations

- **Authors**: Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin
- **Link**: https://arxiv.org/pdf/2310.07849

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 6. Title: Assessing Evaluation Metrics for Neural Test Oracle Generation

- **Authors**: Jiho Shin, Hadi Hemmati, Moshi Wei, Song Wang
- **Link**: https://arxiv.org/pdf/2310.07856

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 7. Title: Pit One Against Many: Leveraging Attention-head Embeddings for  Parameter-efficient Multi-head Attention

- **Authors**: Huiyin Xue, Nikolaos Aletras
- **Link**: https://arxiv.org/pdf/2310.07911

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 8. Title: Harnessing Large Language Models' Empathetic Response Generation  Capabilities for Online Mental Health Counselling Support

- **Authors**: Siyuan Brandon Loh, Aravind Sesagiri Raamkumar
- **Link**: https://arxiv.org/pdf/2310.08017

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 9. Title: Exploring Large Language Models for Multi-Modal Out-of-Distribution  Detection

- **Authors**: Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li
- **Link**: https://arxiv.org/pdf/2310.08027

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 10. Title: QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large  Language Models

- **Authors**: Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan Zhuang
- **Link**: https://arxiv.org/pdf/2310.08041

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 11. Title: Training Generative Question-Answering on Synthetic Data Obtained from  an Instruct-tuned Mo

- **Authors**: Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki
- **Link**: https://arxiv.org/pdf/2310.08072

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 12. Title: To token or not to token: A Comparative Study of Text Representations  for Cross-Lingual Transfer

- **Authors**: Md Mushfiqur Rahman, Fardin Ahsan Sakib, Fahim Faisal, Antonios Anastasopoulos
- **Link**: https://arxiv.org/pdf/2310.08078

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 13. Title: Low-Resource Clickbait Spoiling for Indonesian via Question Answering

- **Authors**: Ni Putu Intan Maharani, Ayu Purwarianti, Alham Fikri Aji
- **Link**: https://arxiv.org/pdf/2310.08085

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 14. Title: ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using  Natural Language Processing

- **Authors**: Ajay Krishnan T. K., V. S. Anoop
- **Link**: https://arxiv.org/pdf/2310.08099

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 15. Title: Promptor: A Conversational and Autonomous Prompt Generation Agent for  Intelligent Text Entry Techniques

- **Authors**: Junxiao Shen, John J. Dudley, Jingyao Zheng, Bill Byrne, Per Ola Kristensson
- **Link**: https://arxiv.org/pdf/2310.08101

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 16. Title: Who Wrote it and Why? Prompting Large-Language Models for Authorship  Verification

- **Authors**: Chia-Yu Hung, Zhiqiang Hu, Yujia Hu, Roy Ka-Wei Lee
- **Link**: https://arxiv.org/pdf/2310.08123

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 17. Title: Context Compression for Auto-regressive Transformers with Sentinel  Tokens

- **Authors**: Siyu Ren, Qi Jia, Kenny Q. Zhu
- **Link**: https://arxiv.org/pdf/2310.08152

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 18. Title: Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task  Instruction Tuning

- **Authors**: Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, Pingjian Zhang
- **Link**: https://arxiv.org/pdf/2310.08166

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 19. Title: Multiclass Classification of Policy Documents with Large Language Models

- **Authors**: Erkan Gunes, Christoffer Koch Florczak
- **Link**: https://arxiv.org/pdf/2310.08167

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 20. Title: Exploring the Cognitive Knowledge Structure of Large Language Models: An  Educational Diagnostic Assessment Approach

- **Authors**: Zheyuan Zhang, Jifan Yu, Juanzi Li, Lei Hou
- **Link**: https://arxiv.org/pdf/2310.08172

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 21. Title: EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form  Narrative Text Generation

- **Authors**: Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, Nan Duan
- **Link**: https://arxiv.org/pdf/2310.08185

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 22. Title: Language Models are Universal Embedders

- **Authors**: Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang
- **Link**: https://arxiv.org/pdf/2310.08232

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 23. Title: Who Said That? Benchmarking Social Media AI Detection

- **Authors**: Wanyun Cui, Linqiu Zhang, Qianle Wang, Shuyang Cai
- **Link**: https://arxiv.org/pdf/2310.08240

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 24. Title: Impact of Co-occurrence on Factual Knowledge of Large Language Models

- **Authors**: Cheongwoong Kang, Jaesik Choi
- **Link**: https://arxiv.org/pdf/2310.08256

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 25. Title: CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large  Language Models

- **Authors**: Rui Yang, Li Fang, Yi Zhou
- **Link**: https://arxiv.org/pdf/2310.08279

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 26. Title: Expanding the Vocabulary of BERT for Knowledge Base Construction

- **Authors**: Dong Yang, Xu Wang, Remzi Celebi
- **Link**: https://arxiv.org/pdf/2310.08291

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 27. Title: Not All Demonstration Examples are Equally Beneficial: Reweighting  Demonstration Examples for In-Context Learning

- **Authors**: Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui
- **Link**: https://arxiv.org/pdf/2310.08309

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 28. Title: From Large Language Models to Knowledge Graphs for Biomarker Discovery  in Cancer

- **Authors**: Md. Rezaul Karim, Lina Molinas Comet, Md Shajalal, Oya Beyan, Dietrich Rebholz-Schuhmann, Stefan Decker
- **Link**: https://arxiv.org/pdf/2310.08365

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 29. Title: Improving Factual Consistency for Knowledge-Grounded Dialogue Systems  via Knowledge Enhancement and Alignment

- **Authors**: Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong
- **Link**: https://arxiv.org/pdf/2310.08372

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 30. Title: Towards Better Evaluation of Instruction-Following: A Case-Study in  Summarization

- **Authors**: Ondrej Skopek, Rahul Aralikatte, Sian Gooding, Victor Carbune
- **Link**: https://arxiv.org/pdf/2310.08394

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 31. Title: Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation

- **Authors**: Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian, Yunshi Lan
- **Link**: https://arxiv.org/pdf/2310.08395

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 32. Title: A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative  Writing

- **Authors**: Carlos Gmez-Rodrguez, Paul Williams
- **Link**: https://arxiv.org/pdf/2310.08433

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 33. Title: DistillSpec: Improving Speculative Decoding via Knowledge Distillation

- **Authors**: Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Franois Kagy, Rishabh Agarwal
- **Link**: https://arxiv.org/pdf/2310.08461

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 34. Title: Can We Edit Multimodal Large Language Models?

- **Authors**: Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang
- **Link**: https://arxiv.org/pdf/2310.08475

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 35. Title: GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language  Models

- **Authors**: Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp
- **Link**: https://arxiv.org/pdf/2310.08487

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 36. Title: Prometheus: Inducing Fine-grained Evaluation Capability in Language  Models

- **Authors**: Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo
- **Link**: https://arxiv.org/pdf/2310.08491

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 37. Title: HoneyBee: Progressive Instruction Finetuning of Large Language Models  for Materials Science

- **Authors**: Yu Song, Santiago Miret, Huan Zhang, Bang Liu
- **Link**: https://arxiv.org/pdf/2310.08511

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 38. Title: LLM-augmented Preference Learning from Natural Language

- **Authors**: Inwon Kang, Sikai Ruan, Tyler Ho, Jui-Chien Lin, Farhad Mohsin, Oshani Seneviratne, Lirong Xia
- **Link**: https://arxiv.org/pdf/2310.08523

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 39. Title: Do pretrained Transformers Really Learn In-context by Gradient Descent?

- **Authors**: Lingfeng Shen, Aayush Mishra, Daniel Khashabi
- **Link**: https://arxiv.org/pdf/2310.08540

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 40. Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of  Language Models with Hypothesis Refinement

- **Authors**: Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren
- **Link**: https://arxiv.org/pdf/2310.08559

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 41. Title: Tree-Planner: Efficient Close-loop Task Planning with Large Language  Models

- **Authors**: Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo
- **Link**: https://arxiv.org/pdf/2310.08582

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 42. Title: Language Models As Semantic Indexers

- **Authors**: Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, Suhang Wang, Jiawei Han, Xianfeng Tang
- **Link**: https://arxiv.org/pdf/2310.07815

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 43. Title: LangNav: Language as a Perceptual Representation for Navigation

- **Authors**: Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim
- **Link**: https://arxiv.org/pdf/2310.07889

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 44. Title: The Expresssive Power of Transformers with Chain of Thought

- **Authors**: William Merrill, Ashish Sabharwal
- **Link**: https://arxiv.org/pdf/2310.07923

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 45. Title: Think, Act, and Ask: Open-World Interactive Personalized Robot  Navigation

- **Authors**: Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
- **Link**: https://arxiv.org/pdf/2310.07968

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 46. Title: Defending Our Privacy With Backdoors

- **Authors**: Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting
- **Link**: https://arxiv.org/pdf/2310.08320

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

## 47. Title: Formally Specifying the High-Level Behavior of LLM-Based Agents

- **Authors**: Maxwell Crouse, Ibrahim Abdelaziz, Kinjal Basu, Soham Dan, Sadhana Kumaravel, Achille Fokoue, Pavan Kapanipathi, Luis Lastras
- **Link**: https://arxiv.org/pdf/2310.08535

- **Abstract**:

Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.

---

